#!/usr/bin/python

"""
Dependencies:
  dateutil.parser - http://labix.org/python-dateutil
  feedparser      - http://www.feedparser.org
  stripogram      - http://www.zope.org/Members/chrisw/StripOGram
  django          - http://www.djangoproject.com
"""

__copyright__ = 'Copyright 2010, Jack Lloyd'
__license__   = 'GPLv2'
__author__    = 'Jack Lloyd'
__version__   = '0.1'
__email__     = 'lloyd@randombit.net'
__url__       = 'http://www.randombit.net/bitbashing/grab_rss'

import logging
import optparse
import os
import smtplib
import socket
import sys
import ConfigParser
import sqlite3
import re
import unicodedata
import time

import feedparser
import stripogram

from email.MIMEText import MIMEText

try:
    import dateutil.parser

    def parse_date(date_string):
        dt = dateutil.parser.parse(date_string)
        return dt.strftime('%Y-%m-%d %H:%M')

except ImportError:
    def parse_date(date_string): return date_string # leave it unchanged

def conf_dir():
    if os.getenv('GRAB_RSS_DIR'):
        return os.getenv('GRAB_RSS_DIR')
    return os.path.join(os.getenv('HOME'), '.grab_rss')

def feedlist():
    feed_file = os.path.join(conf_dir(), 'feeds.txt')

    logging.info('Reading feed list from %s' % (feed_file))

    try:
        return map(lambda s: s.strip(), open(feed_file).readlines())
    except IOError:
        raise Exception('No feeds found in %s' % (feed_file))

def read_config():
    config = ConfigParser.RawConfigParser(
        {
            'from': 'grab-rss@localhost',
            'smtp_host': 'localhost',
            'socket_timeout': '30',
            'user_agent': 'grab_rss ' + __version__ + ' ' + __url__,
            'pool_size': 0
        })

    config_file = os.path.join(conf_dir(), 'grab_rss.conf')

    try:
        config.readfp(open(config_file))
    except IOError:
        pass

    return config

class seen_items:
    def __init__(self, filename):
        self.db = sqlite3.connect(filename)
        self.cursor = self.db.cursor()
        self.cursor.execute('create table if not exists seen (timestamp integer, url text)')

    def save(self):
        self.db.commit()

    def remove_older_than(self, days):
        long_ago = int(time.time()) - int(days)*24*60

        self.cursor.execute('delete from seen where timestamp <= ?', [long_ago])
        return self.cursor.rowcount

    def seen_this_before(self, item):
        self.cursor.execute('select * from seen where url = ?', [item])
        there = self.cursor.fetchall()
        return len(there) > 0

    def note_as_seen(self, item):
        now = int(time.time())
        self.cursor.execute('insert into seen values(?, ?)', [now, item])

def wrap(text, width):
    # From http://code.activestate.com/recipes/148061/
    return reduce(lambda line, word, width=width: '%s%s%s' %
                  (line,
                   ' \n'[(len(line)-line.rfind('\n')-1
                         + len(word.split('\n',1)[0]
                              ) >= width)],
                   word),
                  text.split(' ')
                 )

def force_to_ascii(s):
    def replace_char(matches):
        u = matches.group(1)
        try:
            return unichr(int(u))
        except:
            return u

    s = re.sub("&#(\d+)(;|(?=\s))", replace_char, s)

    return unicodedata.normalize('NFKD', unicode(s)).encode('ascii', 'ignore')

def body_for(entry):
    timestamp = parse_date(entry.get('date', ''))

    link = force_to_ascii(entry.link)

    descr = wrap(stripogram.html2safehtml(
        force_to_ascii(entry.get('description', '')),
        valid_tags=('a')), 80)

    return '\n\n'.join([timestamp, link, descr])

def feed_name(feed, feed_url):
    try:
        return feed['feed']['title']
    except:
        return feed_url.split('/')[3]

def parse_feed(url):
    logging.info('Reading %s' % (url))
    return (url, feedparser.parse(url))

def grab_feeds(state, pool_size):

    if pool_size == 0:
        all_feeds = dict(map(parse_feed, feedlist()))
    else:
        import multiprocessing
        pool = multiprocessing.Pool(pool_size)
        all_feeds = dict(pool.map(parse_feed, feedlist()))

    for (feed_url, feed) in all_feeds.items():
        feed_title = feed_name(feed, feed_url)

        num_entries = len(feed.entries)

        if num_entries:
            logging.debug('Found %d entries in %s' % (num_entries, feed_url))
        else:
            logging.info('Found no entries in %s' % (feed_url))

        new_entries = 0

        for entry in feed.entries:
            if state.seen_this_before(entry.link):
                continue

            title = entry.get('title', entry.link)

            msg = MIMEText(body_for(entry))
            msg['X-GrabRSS-Feed'] = feed_url
            msg['Subject'] = force_to_ascii('%s - %s' % (feed_title, title))
            yield msg

            new_entries += 1

            state.note_as_seen(entry.link)

        logging.debug('Saw %d new items from %s' % (new_entries, feed_url))

def main(argv = None):
    if argv is None:
        argv = sys.argv

    parser = optparse.OptionParser(version='%%prog %s' % (__version__))

    parser.add_option('-v', '--verbose', dest='verbose',
                      action='store_true', default=False,
                      help='be loud')

    parser.add_option('-q', '--quiet', dest='quiet',
                      action='store_true', default=False,
                      help='be quiet')

    parser.add_option('--dont-send', action='store_true', default=False,
                      help="don't actually send email")

    parser.add_option('--remove-older-than', dest='remove_older_than',
                      metavar='N',
                      help='remove seen.db entries older than N days')

    parser.add_option('--pool-size', metavar='N',
                      help='use N processes for pulling down feeds in parallel')

    (options, args) = parser.parse_args(argv)

    def log_level():
        if options.quiet: # -q overrides -v
            return logging.WARNING
        if options.verbose:
            return logging.DEBUG
        return logging.INFO

    logging.basicConfig(stream = sys.stdout,
                        format = '%(levelname) 7s: %(message)s',
                        level = log_level())

    config = read_config()

    socket.setdefaulttimeout(config.getint('GrabRSS', 'socket_timeout'))
    feedparser.USER_AGENT = config.get('GrabRSS', 'user_agent')

    pool_size = int(options.pool_size or config.get('GrabRSS', 'pool_size'))

    socket.setdefaulttimeout(10)

    smtp_host = config.get('GrabRSS', 'smtp_host')

    logging.debug('Sending mail to %s' % (smtp_host))

    state = seen_items(os.path.join(conf_dir(), 'seen.db'))

    if options.remove_older_than:
        removed = state.remove_older_than(options.remove_older_than)
        logging.info('Removed %d old items' % (removed))
        state.save()
        return

    smtp = smtplib.SMTP(smtp_host)

    for email in grab_feeds(state, pool_size):
        if not options.dont_send:
            email['From'] = config.get('GrabRSS', 'from')
            email['To'] = config.get('GrabRSS', 'to')
            smtp.sendmail(email['From'], [email['To']], email.as_string())

    state.save()

    smtp.quit()

if __name__ == '__main__':
    #try:
    sys.exit(main())
    #except Exception, e:
    #    print >>sys.stderr, e
